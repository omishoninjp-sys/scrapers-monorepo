"""
The Maple Mania æ¥“ç³–ç”·å­© å•†å“çˆ¬èŸ² + Shopify ä¸Šæ¶å·¥å…·
åŠŸèƒ½ï¼š
1. çˆ¬å– sucreyshopping.jp The Maple Mania æ‰€æœ‰å•†å“
2. éæ¿¾ 1000å††ä»¥ä¸‹å•†å“ã€é»æ•¸å•†å“
3. ä¸Šæ¶åˆ° Shopifyï¼ˆä¸é‡è¤‡ä¸Šæ¶ï¼‰
4. åŸåƒ¹å¯«å…¥æˆæœ¬åƒ¹ï¼ˆCostï¼‰
5. OpenAI ç¿»è­¯æˆç¹é«”ä¸­æ–‡
6. SEO å’Œ GEO å„ªåŒ–
7. ä¸è¨­å®šåº«å­˜æ•¸é‡
8. ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®æ¸ é“
"""

from flask import Flask, jsonify, request
import requests
from bs4 import BeautifulSoup
import re
import json
import os
import time
from urllib.parse import urljoin
import math
import threading
import base64

app = Flask(__name__)

# ========== è¨­å®š ==========
SHOPIFY_SHOP = ""  # å¾ç’°å¢ƒè®Šæ•¸è®€å–
SHOPIFY_ACCESS_TOKEN = ""  # å¾ç’°å¢ƒè®Šæ•¸è®€å–

BASE_URL = "https://sucreyshopping.jp"
# å•†å“åˆ—è¡¨é é¢ (åˆ†é )
LIST_PAGES = [
    "https://sucreyshopping.jp/shop/c/c10/?brand=themaplemania",
    "https://sucreyshopping.jp/shop/c/c10_p2/?brand=themaplemania",
    "https://sucreyshopping.jp/shop/c/c10_p3/?brand=themaplemania",
    "https://sucreyshopping.jp/shop/c/c10_p4/?brand=themaplemania",
]

# å“ç‰Œå‰ç¶´
BRAND_PREFIX = "The maple mania æ¥“ç³–ç”·å­©"

# æœ€ä½åƒ¹æ ¼é–€æª»ï¼ˆæ—¥å¹£ï¼‰
MIN_PRICE = 1000

# OpenAI API è¨­å®š (å¾ç’°å¢ƒè®Šæ•¸è®€å–)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

# æ¨¡æ“¬ç€è¦½å™¨ Headers
BROWSER_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
}

# å»ºç«‹ Session
session = requests.Session()
session.headers.update(BROWSER_HEADERS)

# å…¨åŸŸè®Šæ•¸å­˜å„²çˆ¬å–ç‹€æ…‹
scrape_status = {
    "running": False,
    "progress": 0,
    "total": 0,
    "current_product": "",
    "products": [],
    "errors": [],
    "uploaded": 0,
    "skipped": 0,
    "skipped_low_price": 0,
    "skipped_points": 0,
    "skipped_exists": 0,
    "filtered_by_price": 0,
    "deleted": 0
}


def load_shopify_token():
    """è¼‰å…¥ Shopify Access Token å’Œå•†åº—åç¨± (å„ªå…ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–)"""
    global SHOPIFY_ACCESS_TOKEN, SHOPIFY_SHOP
    
    # å„ªå…ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–
    env_token = os.environ.get('SHOPIFY_ACCESS_TOKEN', '')
    env_shop = os.environ.get('SHOPIFY_SHOP', '')
    
    if env_token and env_shop:
        SHOPIFY_ACCESS_TOKEN = env_token
        SHOPIFY_SHOP = env_shop.replace('https://', '').replace('http://', '').replace('.myshopify.com', '').strip('/')
        print(f"[è¨­å®š] å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥ - å•†åº—: {SHOPIFY_SHOP}")
        return True
    
    # å‚™ç”¨ï¼šå¾æª”æ¡ˆè®€å–
    token_file = "shopify_token.json"
    if os.path.exists(token_file):
        with open(token_file, 'r') as f:
            data = json.load(f)
            SHOPIFY_ACCESS_TOKEN = data.get('access_token', '')
            shop = data.get('shop', '')
            if shop:
                SHOPIFY_SHOP = shop.replace('https://', '').replace('http://', '').replace('.myshopify.com', '').strip('/')
            
            print(f"[è¨­å®š] å¾æª”æ¡ˆè¼‰å…¥ - å•†åº—: {SHOPIFY_SHOP}")
            return True
    return False
    return False


def get_shopify_headers():
    """å–å¾— Shopify API Headers"""
    return {
        'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN,
        'Content-Type': 'application/json',
    }


def shopify_api_url(endpoint):
    """å»ºç«‹ Shopify API URL"""
    return f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-01/{endpoint}"


def calculate_selling_price(cost, weight):
    """
    è¨ˆç®—å”®åƒ¹
    å…¬å¼ï¼š[é€²è²¨åƒ¹ + (é‡é‡ * 1250)] / 0.7 = å”®åƒ¹
    """
    if not cost or cost <= 0:
        return 0
    
    shipping_cost = weight * 1250 if weight else 0
    price = (cost + shipping_cost) / 0.7
    
    return round(price)


def clean_html_for_translation(html_text):
    """æ¸…é™¤ HTML ä¸­çš„ CSSã€script å’Œå¤šé¤˜æ¨™ç±¤ï¼Œåªä¿ç•™ç´”æ–‡å­—"""
    if not html_text:
        return ""
    
    text = html_text
    
    # ç§»é™¤ style æ¨™ç±¤åŠå…§å®¹
    text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # ç§»é™¤ script æ¨™ç±¤åŠå…§å®¹
    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # ç§»é™¤ CSS æ¨£å¼å¡Š
    text = re.sub(r'#[\w-]+\s*\{[^}]*\}', '', text, flags=re.DOTALL)
    text = re.sub(r'\.[\w-]+\s*\{[^}]*\}', '', text, flags=re.DOTALL)
    text = re.sub(r'@media[^{]*\{[^}]*\}', '', text, flags=re.DOTALL)
    
    # ç§»é™¤å…§è¯ style å±¬æ€§
    text = re.sub(r'\s*style\s*=\s*["\'][^"\']*["\']', '', text, flags=re.IGNORECASE)
    
    # ç§»é™¤ HTML æ¨™ç±¤ï¼Œä¿ç•™æ›è¡Œ
    text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
    text = re.sub(r'</p>', '\n', text, flags=re.IGNORECASE)
    text = re.sub(r'</div>', '\n', text, flags=re.IGNORECASE)
    text = re.sub(r'<[^>]+>', '', text)
    
    # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å’Œæ›è¡Œ
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    return text.strip()


def translate_with_chatgpt(title, description):
    """
    ä½¿ç”¨ ChatGPT ç¿»è­¯å•†å“åç¨±å’Œèªªæ˜ï¼Œä¸¦ç”Ÿæˆ SEO å…§å®¹
    å›å‚³ï¼štranslated_title, translated_description, page_title, meta_description
    """
    # æ¸…ç†æè¿°ä¸­çš„ HTML/CSS
    clean_description = clean_html_for_translation(description)
    
    # å…ˆç§»é™¤æè¿°ä¸­çš„åƒ¹æ ¼è³‡è¨Š
    clean_description = re.sub(r'[\d,]+\s*å††', '', clean_description)
    clean_description = re.sub(r'åƒ¹æ ¼[ï¼š:]\s*[\d,]+\s*æ—¥åœ“', '', clean_description)
    clean_description = re.sub(r'ä¾¡æ ¼[ï¼š:]\s*[\d,]+', '', clean_description)
    clean_description = re.sub(r'ç¨è¾¼[\d,]+å††', '', clean_description)
    clean_description = re.sub(r'[\d,]+å††\s*ï¼ˆç¨è¾¼ï¼‰', '', clean_description)
    
    prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æœ¬ç”œé»å•†å“è³‡è¨Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼Œä¸¦å„ªåŒ– SEOã€‚

å•†å“åç¨±ï¼ˆæ—¥æ–‡ï¼‰ï¼š{title}
å•†å“èªªæ˜ï¼ˆæ—¥æ–‡ï¼‰ï¼š{clean_description[:1500]}

è«‹å›å‚³ JSON æ ¼å¼ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š
{{
    "title": "ç¿»è­¯å¾Œçš„å•†å“åç¨±ï¼ˆç¹é«”ä¸­æ–‡ï¼Œç°¡æ½”æœ‰åŠ›ï¼Œå‰é¢åŠ ä¸Š The maple mania æ¥“ç³–ç”·å­©ï¼‰",
    "description": "ç¿»è­¯å¾Œçš„å•†å“èªªæ˜ï¼ˆç¹é«”ä¸­æ–‡ï¼Œä¿ç•™åŸæ„ä½†æ›´æµæš¢ï¼Œé©åˆé›»å•†å±•ç¤ºï¼Œä½¿ç”¨ HTML æ ¼å¼ï¼‰",
    "page_title": "SEO é é¢æ¨™é¡Œï¼ˆç¹é«”ä¸­æ–‡ï¼ŒåŒ…å« The maple mania æ¥“ç³–ç”·å­©å“ç‰Œå’Œå•†å“ç‰¹è‰²ï¼Œ50-60å­—ä»¥å…§ï¼‰",
    "meta_description": "SEO æè¿°ï¼ˆç¹é«”ä¸­æ–‡ï¼Œå¸å¼•é»æ“Šï¼ŒåŒ…å«é—œéµå­—ï¼Œ100å­—ä»¥å…§ï¼‰"
}}

æ³¨æ„ï¼š
1. é€™æ˜¯æ—¥æœ¬ The Maple Mania æ¥“ç³–ç”·å­©çš„æ¥“ç³–ç”œé»ï¼ˆé¤…ä¹¾ã€è²»å—é›ªã€å¹´è¼ªè›‹ç³•ï¼‰
2. ç¿»è­¯è¦è‡ªç„¶æµæš¢ï¼Œä¸è¦ç”Ÿç¡¬
3. å•†å“æ¨™é¡Œé–‹é ­å¿…é ˆæ˜¯ã€ŒThe maple mania æ¥“ç³–ç”·å­©ã€
4. SEO å…§å®¹è¦åŒ…å«ï¼šThe maple maniaã€æ¥“ç³–ç”·å­©ã€æ—¥æœ¬ã€æ±äº¬ä¼´æ‰‹ç¦®ã€é€ç¦®ã€æ¥“ç³–é¤…ä¹¾ç­‰é—œéµå­—
5. æ¥“ç³–ç”·å­©æ˜¯æ±äº¬è»Šç«™æœ€å—æ­¡è¿çš„ä¼´æ‰‹ç¦®ä¹‹ä¸€
6. æè¿°ä¸­å¯ä»¥æåˆ°å°ç£ä»£è³¼ã€æ—¥æœ¬ç›´é€ç­‰é—œéµå­—ï¼Œå¢åŠ  SEO æ•ˆæœ
7. **é‡è¦ï¼šèªªæ˜æ–‡ä¸­ä¸è¦åŒ…å«ä»»ä½•åƒ¹æ ¼è³‡è¨Šï¼ˆå¦‚ã€Œxxxå††ã€ã€Œxxxæ—¥åœ“ã€ç­‰ï¼‰**
8. åªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—"""

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ï¼Œå°ˆé–€è™•ç†æ—¥æœ¬é«˜ç´šç”œé»çš„ä¸­æ–‡ç¿»è­¯ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0,
                "max_tokens": 1000
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # æ¸…ç†å¯èƒ½çš„ markdown æ¨™è¨˜
            content = content.strip()
            if content.startswith('```'):
                content = content.split('\n', 1)[1]
            if content.endswith('```'):
                content = content.rsplit('```', 1)[0]
            content = content.strip()
            
            # è§£æ JSON
            translated = json.loads(content)
            
            # ç¢ºä¿æ¨™é¡Œé–‹é ­æœ‰å“ç‰Œå
            trans_title = translated.get('title', title)
            if not trans_title.startswith('The maple mania') and not trans_title.startswith('The Maple Mania'):
                trans_title = f"{BRAND_PREFIX} {trans_title}"
            
            # æ¸…é™¤æè¿°ä¸­å¯èƒ½æ®˜ç•™çš„åƒ¹æ ¼è³‡è¨Š
            trans_desc = translated.get('description', description)
            trans_desc = re.sub(r'[\d,]+\s*å††', '', trans_desc)
            trans_desc = re.sub(r'[\d,]+\s*æ—¥åœ“', '', trans_desc)
            trans_desc = re.sub(r'åƒ¹æ ¼[ï¼š:]\s*[\d,]+', '', trans_desc)
            trans_desc = re.sub(r'ä¾¡æ ¼[ï¼š:]\s*[\d,]+', '', trans_desc)
            
            return {
                'success': True,
                'title': trans_title,
                'description': trans_desc,
                'page_title': translated.get('page_title', ''),
                'meta_description': translated.get('meta_description', '')
            }
        else:
            print(f"[OpenAI éŒ¯èª¤] {response.status_code}: {response.text}")
            return {
                'success': False,
                'title': f"{BRAND_PREFIX} {title}",
                'description': description,
                'page_title': '',
                'meta_description': ''
            }
            
    except Exception as e:
        print(f"[ç¿»è­¯éŒ¯èª¤] {e}")
        return {
            'success': False,
            'title': f"{BRAND_PREFIX} {title}",
            'description': description,
            'page_title': '',
            'meta_description': ''
        }


def download_image_to_base64(img_url, max_retries=3):
    """
    ä¸‹è¼‰åœ–ç‰‡ä¸¦è½‰æ›ç‚º Base64
    ä½¿ç”¨èˆ‡ç€è¦½å™¨ç›¸åŒçš„ headers ä¾†é¿å…é˜²ç›œé€£
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Referer': 'https://sucreyshopping.jp/',
        'Connection': 'keep-alive',
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(img_url, headers=headers, timeout=30)
            if response.status_code == 200:
                # å–å¾—åœ–ç‰‡æ ¼å¼
                content_type = response.headers.get('Content-Type', 'image/jpeg')
                if 'jpeg' in content_type or 'jpg' in content_type:
                    img_format = 'image/jpeg'
                elif 'png' in content_type:
                    img_format = 'image/png'
                elif 'webp' in content_type:
                    img_format = 'image/webp'
                elif 'gif' in content_type:
                    img_format = 'image/gif'
                else:
                    img_format = 'image/jpeg'  # é è¨­
                
                # è½‰æ›ç‚º Base64
                img_base64 = base64.b64encode(response.content).decode('utf-8')
                return {
                    'success': True,
                    'base64': img_base64,
                    'content_type': img_format
                }
            else:
                print(f"[åœ–ç‰‡ä¸‹è¼‰] ç¬¬ {attempt+1} æ¬¡å˜—è©¦å¤±æ•—: HTTP {response.status_code}")
        except Exception as e:
            print(f"[åœ–ç‰‡ä¸‹è¼‰] ç¬¬ {attempt+1} æ¬¡å˜—è©¦ç•°å¸¸: {e}")
        
        time.sleep(1)  # é‡è©¦å‰ç­‰å¾…
    
    return {'success': False}


def get_existing_skus():
    """å–å¾— Shopify å·²å­˜åœ¨çš„ SKU åˆ—è¡¨ï¼ˆåªå›å‚³ SKU setï¼Œå‘ä¸‹ç›¸å®¹ï¼‰"""
    products_map = get_existing_products_map()
    return set(products_map.keys())

def get_existing_products_map():
    """å–å¾— Shopify å·²å­˜åœ¨çš„å•†å“ï¼Œå›å‚³ {sku: product_id} å­—å…¸"""
    products_map = {}
    url = shopify_api_url("products.json?limit=250")
    
    while url:
        response = requests.get(url, headers=get_shopify_headers())
        if response.status_code != 200:
            print(f"Error fetching products: {response.status_code}")
            break
        
        data = response.json()
        for product in data.get('products', []):
            product_id = product.get('id')
            for variant in product.get('variants', []):
                sku = variant.get('sku')
                if sku and product_id:
                    products_map[sku] = product_id
        
        link_header = response.headers.get('Link', '')
        if 'rel="next"' in link_header:
            match = re.search(r'<([^>]+)>; rel="next"', link_header)
            if match:
                url = match.group(1)
            else:
                url = None
        else:
            url = None
    
    return products_map

def get_collection_products_map(collection_id):
    """åªå–å¾—ç‰¹å®š Collection å…§çš„å•†å“ï¼Œå›å‚³ {sku: product_id} å­—å…¸"""
    products_map = {}
    if not collection_id:
        return products_map
    
    url = shopify_api_url(f"collections/{collection_id}/products.json?limit=250")
    
    while url:
        response = requests.get(url, headers=get_shopify_headers())
        if response.status_code != 200:
            break
        
        data = response.json()
        for product in data.get('products', []):
            product_id = product.get('id')
            for variant in product.get('variants', []):
                sku = variant.get('sku')
                if sku and product_id:
                    products_map[sku] = product_id
        
        link_header = response.headers.get('Link', '')
        if 'rel="next"' in link_header:
            match = re.search(r'<([^>]+)>; rel="next"', link_header)
            if match:
                url = match.group(1)
            else:
                url = None
        else:
            url = None
    
    print(f"[INFO] Collection å…§æœ‰ {len(products_map)} å€‹å•†å“")
    return products_map

def set_product_to_draft(product_id):
    """å°‡ Shopify å•†å“è¨­ç‚ºè‰ç¨¿"""
    url = shopify_api_url(f"products/{product_id}.json")
    response = requests.put(url, headers=get_shopify_headers(), json={
        "product": {"id": product_id, "status": "draft"}
    })
    if response.status_code == 200:
        print(f"[è¨­ç‚ºè‰ç¨¿] Product ID: {product_id}")
        return True
    return False


def get_or_create_collection(collection_title="The maple mania æ¥“ç³–ç”·å­©"):
    """å–å¾—æˆ–å»ºç«‹ Collection"""
    response = requests.get(
        shopify_api_url(f'custom_collections.json?title={collection_title}'),
        headers=get_shopify_headers()
    )
    
    if response.status_code == 200:
        collections = response.json().get('custom_collections', [])
        for col in collections:
            if col['title'] == collection_title:
                print(f"[INFO] æ‰¾åˆ°ç¾æœ‰ Collection: {collection_title} (ID: {col['id']})")
                return col['id']
    
    # ä¸å­˜åœ¨å‰‡å»ºç«‹
    response = requests.post(
        shopify_api_url('custom_collections.json'),
        headers=get_shopify_headers(),
        json={
            'custom_collection': {
                'title': collection_title,
                'published': True
            }
        }
    )
    
    if response.status_code == 201:
        collection_id = response.json()['custom_collection']['id']
        print(f"[INFO] å»ºç«‹æ–° Collection: {collection_title} (ID: {collection_id})")
        return collection_id
    
    print(f"[ERROR] ç„¡æ³•å»ºç«‹ Collection: {response.text}")
    return None


def add_product_to_collection(product_id, collection_id):
    """å°‡å•†å“åŠ å…¥ Collection"""
    response = requests.post(
        shopify_api_url('collects.json'),
        headers=get_shopify_headers(),
        json={
            'collect': {
                'product_id': product_id,
                'collection_id': collection_id
            }
        }
    )
    return response.status_code == 201


def publish_to_all_channels(product_id):
    """ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®æ¸ é“ï¼ˆä½¿ç”¨ GraphQLï¼‰"""
    print(f"[ç™¼å¸ƒ] æ­£åœ¨ç™¼å¸ƒå•†å“ {product_id} åˆ°æ‰€æœ‰æ¸ é“...")
    
    graphql_url = f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-01/graphql.json"
    headers = {
        'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN,
        'Content-Type': 'application/json',
    }
    
    # å…ˆç”¨ GraphQL æŸ¥è©¢æ‰€æœ‰å¯ç™¼å¸ƒçš„æ¸ é“
    query = """
    {
      publications(first: 20) {
        edges {
          node {
            id
            name
            supportsFuturePublishing
          }
        }
      }
    }
    """
    
    response = requests.post(graphql_url, headers=headers, json={'query': query})
    
    if response.status_code != 200:
        print(f"[ç™¼å¸ƒ] ç„¡æ³•å–å¾—æ¸ é“åˆ—è¡¨: {response.status_code}")
        return False
    
    result = response.json()
    publications = result.get('data', {}).get('publications', {}).get('edges', [])
    
    # éæ¿¾å‡ºå”¯ä¸€çš„æ¸ é“ï¼ˆå»é‡ï¼‰
    seen_names = set()
    unique_publications = []
    for pub in publications:
        name = pub['node']['name']
        if name not in seen_names:
            seen_names.add(name)
            unique_publications.append(pub['node'])
    
    print(f"[ç™¼å¸ƒ] æ‰¾åˆ° {len(unique_publications)} å€‹å”¯ä¸€éŠ·å”®æ¸ é“: {[p['name'] for p in unique_publications]}")
    
    # å»ºç«‹ç™¼å¸ƒè«‹æ±‚
    publication_inputs = [{"publicationId": pub['id']} for pub in unique_publications]
    
    mutation = """
    mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
      publishablePublish(id: $id, input: $input) {
        publishable {
          availablePublicationsCount {
            count
          }
          ... on Product {
            publishedOnCurrentPublication
          }
        }
        shop {
          publicationCount
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    variables = {
        "id": f"gid://shopify/Product/{product_id}",
        "input": publication_inputs
    }
    
    pub_response = requests.post(graphql_url, headers=headers, json={
        'query': mutation,
        'variables': variables
    })
    
    if pub_response.status_code == 200:
        pub_result = pub_response.json()
        
        data = pub_result.get('data') or {}
        publishable_publish = data.get('publishablePublish') or {}
        errors = publishable_publish.get('userErrors') or []
        publishable = publishable_publish.get('publishable') or {}
        available_count_obj = publishable.get('availablePublicationsCount') or {}
        available_count = available_count_obj.get('count', 0)
        
        if errors:
            real_errors = [e for e in errors if 'does not exist' not in e.get('message', '')]
            if real_errors:
                print(f"[ç™¼å¸ƒ] éŒ¯èª¤: {real_errors}")
        
        print(f"[ç™¼å¸ƒ] æˆåŠŸç™¼å¸ƒåˆ° {available_count} å€‹æ¸ é“")
        return True
    else:
        print(f"[ç™¼å¸ƒ] GraphQL è«‹æ±‚å¤±æ•—: {pub_response.status_code}")
        return False


def parse_size_weight(text):
    """è§£æå°ºå¯¸å’Œé‡é‡"""
    dimension = None
    weight_kg = None
    
    # æ¨™æº–åŒ–æ–‡å­— (å…¨å½¢è½‰åŠå½¢)
    text = text.replace('Ã—', 'x').replace('ï¼¸', 'x').replace('ï½˜', 'x')
    text = text.replace('ï½ï½', 'mm').replace('ï½‡', 'g').replace('ï½‹ï½‡', 'kg')
    text = text.replace('Î¦', 'x')  # åœ“å½¢ç›´å¾‘ç¬¦è™Ÿ
    text = text.replace(',', '')  # ç§»é™¤åƒåˆ†ä½é€—è™Ÿ
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')  # å…¨å½¢æ‹¬è™Ÿè½‰åŠå½¢
    
    # è§£æå°ºå¯¸ (æ”¯æ´å¤šç¨®æ ¼å¼)
    dim_patterns = [
        # W277Ã— D258Ã— H48(mm) æ ¼å¼
        r'W\s*(\d+(?:\.\d+)?)\s*[xXÃ—]\s*D\s*(\d+(?:\.\d+)?)\s*[xXÃ—]\s*H\s*(\d+(?:\.\d+)?)',
        # æ¨™æº–ç«‹æ–¹é«”æ ¼å¼: 283x205x58mm
        r'(\d+(?:\.\d+)?)\s*[xXÃ—]\s*(\d+(?:\.\d+)?)\s*[xXÃ—]\s*(\d+(?:\.\d+)?)\s*(?:\(?\s*mm\s*\)?)?',
        r'(\d+)\s*[xXÃ—]\s*(\d+)\s*[xXÃ—]\s*(\d+)',
    ]
    
    for pattern in dim_patterns:
        dim_match = re.search(pattern, text, re.IGNORECASE)
        if dim_match:
            l, w, h = float(dim_match.group(1)), float(dim_match.group(2)), float(dim_match.group(3))
            volume_weight = (l * w * h) / 6000000
            volume_weight = round(volume_weight, 2)
            dimension = {"l": l, "w": w, "h": h, "volume_weight": volume_weight}
            print(f"[parse_size_weight] å°ºå¯¸: {l}x{w}x{h}mm -> æç©é‡é‡: {volume_weight}kg")
            break
    
    # è§£æé‡é‡ (kg æˆ– g)
    weight_kg_match = re.search(r'(\d+(?:\.\d+)?)\s*kg', text, re.IGNORECASE)
    weight_g_match = re.search(r'(\d+(?:\.\d+)?)\s*g(?![\w])', text)
    
    if weight_kg_match:
        weight_kg = float(weight_kg_match.group(1))
        print(f"[parse_size_weight] å¯¦éš›é‡é‡: {weight_kg}kg")
    elif weight_g_match:
        weight_kg = float(weight_g_match.group(1)) / 1000
        print(f"[parse_size_weight] å¯¦éš›é‡é‡: {weight_g_match.group(1)}g = {weight_kg}kg")
    
    # å–è¼ƒå¤§å€¼ï¼Œå¦‚æœæ²’æœ‰å¯¦éš›é‡é‡å‰‡ç”¨æç©é‡é‡
    final_weight = 0
    if dimension and weight_kg:
        final_weight = max(dimension.get('volume_weight', 0), weight_kg)
        print(f"[parse_size_weight] å–è¼ƒå¤§å€¼: max({dimension.get('volume_weight', 0)}, {weight_kg}) = {final_weight}kg")
    elif dimension:
        # æ²’æœ‰å¯¦éš›é‡é‡ï¼Œä½¿ç”¨æç©é‡é‡
        final_weight = dimension.get('volume_weight', 0)
        print(f"[parse_size_weight] ç„¡å¯¦éš›é‡é‡ï¼Œä½¿ç”¨æç©é‡é‡: {final_weight}kg")
    elif weight_kg:
        final_weight = weight_kg
    
    return {
        "dimension": dimension,
        "actual_weight": weight_kg,
        "final_weight": round(final_weight, 2)
    }


def scrape_product_list():
    """çˆ¬å–å•†å“åˆ—è¡¨ï¼ˆç´” requests ç‰ˆæœ¬ï¼‰"""
    products = []
    seen_skus = set()
    
    for page_url in LIST_PAGES:
        print(f"[INFO] æ­£åœ¨è¼‰å…¥é é¢: {page_url}")
        
        try:
            response = session.get(page_url, timeout=30)
            if response.status_code != 200:
                print(f"[ERROR] ç„¡æ³•è¼‰å…¥é é¢: {page_url} (HTTP {response.status_code})")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ‰¾æ‰€æœ‰å•†å“é€£çµ /shop/g/g{sku}/
            product_links = soup.select('a[href*="/shop/g/g"]')
            print(f"[INFO] é é¢æ‰¾åˆ° {len(product_links)} å€‹é€£çµ")
            
            for link in product_links:
                try:
                    href = link.get('href', '')
                    if not href:
                        continue
                    
                    # å¾ URL æå– SKU
                    sku_match = re.search(r'/shop/g/g([^/]+)/', href)
                    if not sku_match:
                        continue
                    
                    sku = sku_match.group(1)
                    
                    # è·³éå·²è™•ç†çš„
                    if sku in seen_skus:
                        continue
                    seen_skus.add(sku)
                    
                    # å˜—è©¦å¾åˆ—è¡¨é é¢å–å¾—åƒ¹æ ¼
                    price = 0
                    
                    # æ‰¾åˆ°çˆ¶å…ƒç´ å–å¾—åƒ¹æ ¼
                    parent = link.find_parent(['dl', 'div', 'li'])
                    if parent:
                        parent_text = parent.get_text()
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºé»æ•¸å•†å“
                        if 'ãƒã‚¤ãƒ³ãƒˆ' in parent_text and 'å††' not in parent_text:
                            print(f"[è·³é] é»æ•¸å•†å“: {sku}")
                            continue
                        
                        # æå–åƒ¹æ ¼
                        price_match = re.search(r'([\d,]+)å††', parent_text)
                        if price_match:
                            price = int(price_match.group(1).replace(',', ''))
                        
                        # æª¢æŸ¥å•†å“åç¨±æ˜¯å¦åŒ…å«ã€ŒãŠæ€¥ãä¾¿ã€
                        if 'ãŠæ€¥ãä¾¿' in parent_text:
                            print(f"[è·³é] ãŠæ€¥ãä¾¿å•†å“: {sku}")
                            continue
                    
                    # æª¢æŸ¥æœ€ä½åƒ¹æ ¼
                    if price > 0 and price < MIN_PRICE:
                        print(f"[è·³é] åƒ¹æ ¼éä½: {sku} (Â¥{price})")
                        continue
                    
                    full_url = urljoin(BASE_URL, href)
                    products.append({
                        'url': full_url,
                        'sku': sku,
                        'list_price': price
                    })
                    print(f"[æ”¶é›†] {sku} - Â¥{price}")
                    
                except Exception as e:
                    continue
            
            time.sleep(1)
            
        except Exception as e:
            print(f"[ERROR] è¼‰å…¥é é¢å¤±æ•—: {page_url} - {e}")
            continue
    
    print(f"[INFO] å…±æ”¶é›† {len(products)} å€‹å•†å“")
    return products


def scrape_product_detail(url, max_retries=3):
    """çˆ¬å–å–®ä¸€å•†å“è©³ç´°è³‡è¨Šï¼ˆç´” requests ç‰ˆæœ¬ï¼‰"""
    product = {
        'url': url,
        'title': '',
        'price': 0,
        'description': '',
        'size_weight_text': '',
        'weight': 0,
        'images': [],
        'sku': '',
        'is_points': False
    }
    
    sku_match = re.search(r'/shop/g/g([^/]+)/', url)
    if sku_match:
        product['sku'] = sku_match.group(1)
    
    for attempt in range(max_retries):
        try:
            print(f"[è¼‰å…¥] {url} (å˜—è©¦ {attempt+1}/{max_retries})")
            
            response = session.get(url, timeout=30)
            if response.status_code != 200:
                print(f"[ERROR] HTTP {response.status_code}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            page_text = soup.get_text()
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé»æ•¸å•†å“
            if 'ãƒã‚¤ãƒ³ãƒˆ' in page_text and re.search(r'\d+ãƒã‚¤ãƒ³ãƒˆ', page_text):
                if not re.search(r'[\d,]+å††', page_text):
                    product['is_points'] = True
                    print(f"[è·³é] é»æ•¸å•†å“")
                    return product
            
            # å•†å“åç¨±
            title_selectors = ['h1.goods-name', 'h1[class*="goods"]', '.goods-detail h1', 'h1']
            for sel in title_selectors:
                title_el = soup.select_one(sel)
                if title_el:
                    title_text = title_el.get_text(strip=True)
                    if title_text and len(title_text) > 2:
                        product['title'] = title_text
                        print(f"[æ¨™é¡Œ] {title_text}")
                        break
            
            # åƒ¹æ ¼
            price_selectors = ['.block-goods-price--price', '.js-enhanced-ecommerce-goods-price', '.price']
            for sel in price_selectors:
                price_el = soup.select_one(sel)
                if price_el:
                    price_text = price_el.get_text()
                    price_match = re.search(r'([\d,]+)', price_text)
                    if price_match:
                        product['price'] = int(price_match.group(1).replace(',', ''))
                        print(f"[åƒ¹æ ¼] Â¥{product['price']}")
                        break
            
            # å¾é é¢æ–‡å­—æå–åƒ¹æ ¼ï¼ˆå‚™ç”¨ï¼‰
            if not product['price']:
                price_match = re.search(r'([\d,]+)\s*å††', page_text)
                if price_match:
                    product['price'] = int(price_match.group(1).replace(',', ''))
                    print(f"[åƒ¹æ ¼] Â¥{product['price']} (å¾é é¢æ–‡å­—)")
            
            # å•†å“èªªæ˜
            desc_selectors = ['.goods-description', '.item-description', '.product-description']
            for sel in desc_selectors:
                desc_el = soup.select_one(sel)
                if desc_el:
                    product['description'] = str(desc_el)
                    print(f"[æè¿°] å·²å–å¾—")
                    break
            
            # é‡é‡å’Œå°ºå¯¸
            for dl in soup.select('dl'):
                dl_text = dl.get_text()
                if 'ç®±ã‚µã‚¤ã‚º' in dl_text or 'ã‚µã‚¤ã‚º' in dl_text:
                    dd_el = dl.select_one('dd')
                    if dd_el:
                        dd_text = dd_el.get_text()
                        print(f"[å°ºå¯¸] æ‰¾åˆ°: {dd_text}")
                        product['size_weight_text'] = dd_text
                        weight_info = parse_size_weight(dd_text)
                        if weight_info['final_weight'] > 0:
                            product['weight'] = weight_info['final_weight']
                            print(f"[é‡é‡] {product['weight']}kg")
                            break
            
            # å¾æ•´å€‹é é¢æ‰¾å°ºå¯¸æ ¼å¼ï¼ˆå‚™ç”¨ï¼‰
            if product['weight'] == 0:
                size_match = re.search(r'W\s*(\d+)\s*[Ã—xX]\s*D\s*(\d+)\s*[Ã—xX]\s*H\s*(\d+)', page_text)
                if size_match:
                    l, w, h = float(size_match.group(1)), float(size_match.group(2)), float(size_match.group(3))
                    product['weight'] = round((l * w * h) / 6000000, 2)
                    print(f"[é‡é‡] æç©é‡é‡: {product['weight']}kg")
            
            # æ‰¾ç´”é‡é‡ï¼ˆå‚™ç”¨ï¼‰
            if product['weight'] == 0:
                weight_match = re.search(r'(\d+(?:,\d+)?)\s*[gG](?!ift)', page_text)
                if weight_match:
                    weight_str = weight_match.group(1).replace(',', '')
                    product['weight'] = round(float(weight_str) / 1000, 2)
                    print(f"[é‡é‡] {product['weight']}kg")
            
            # é è¨­é‡é‡
            if product['weight'] == 0:
                product['weight'] = 0.5
                print(f"[é‡é‡] ä½¿ç”¨é è¨­: 0.5kg")
            
            # åœ–ç‰‡
            images = []
            for img in soup.select('img[src*="/img/goods/"]'):
                src = img.get('src') or img.get('data-src')
                if src:
                    src = src.replace('/S/', '/L/').replace('/M/', '/L/')
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif not src.startswith('http'):
                        src = urljoin(BASE_URL, src)
                    if src not in images:
                        images.append(src)
            
            # OG image å‚™ç”¨
            if not images:
                og_img = soup.select_one('meta[property="og:image"]')
                if og_img:
                    src = og_img.get('content')
                    if src:
                        if not src.startswith('http'):
                            src = urljoin(BASE_URL, src)
                        images.append(src)
            
            product['images'] = images[:10]
            print(f"[åœ–ç‰‡] å–å¾— {len(product['images'])} å¼µ")
            
            return product
            
        except Exception as e:
            print(f"[ERROR] çˆ¬å–å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(3)
    
    return product


def upload_to_shopify(product, collection_id=None):
    """ä¸Šå‚³å•†å“åˆ° Shopify"""
    
    # ç¿»è­¯å•†å“åç¨±å’Œèªªæ˜
    original_title = product['title']
    
    print(f"[ç¿»è­¯] æ­£åœ¨ç¿»è­¯: {original_title[:30]}...")
    translated = translate_with_chatgpt(original_title, product.get('description', ''))
    
    if translated['success']:
        print(f"[ç¿»è­¯æˆåŠŸ] {translated['title'][:30]}...")
    else:
        print(f"[ç¿»è­¯å¤±æ•—] ä½¿ç”¨åŸæ–‡")
    
    # è¨ˆç®—å”®åƒ¹
    cost = product['price']
    weight = product.get('weight', 0)
    selling_price = calculate_selling_price(cost, weight)
    
    print(f"[åƒ¹æ ¼è¨ˆç®—] é€²è²¨åƒ¹: Â¥{cost}, é‡é‡: {weight}kg, å”®åƒ¹: Â¥{selling_price}")
    
    # ä¸‹è¼‰åœ–ç‰‡ä¸¦è½‰æ›ç‚º Base64
    images_base64 = []
    img_urls = product.get('images', [])
    print(f"[åœ–ç‰‡] é–‹å§‹ä¸‹è¼‰ {len(img_urls)} å¼µåœ–ç‰‡...")
    
    for idx, img_url in enumerate(img_urls):
        if not img_url or not img_url.startswith('http'):
            continue
        
        print(f"[åœ–ç‰‡] ä¸‹è¼‰ä¸­ ({idx+1}/{len(img_urls)}): {img_url[:60]}...")
        result = download_image_to_base64(img_url)
        
        if result['success']:
            images_base64.append({
                'attachment': result['base64'],
                'position': idx + 1,
                'filename': f"maple_mania_{product['sku']}_{idx+1}.jpg"
            })
            print(f"[åœ–ç‰‡] âœ“ ä¸‹è¼‰æˆåŠŸ ({idx+1}/{len(img_urls)})")
        else:
            print(f"[åœ–ç‰‡] âœ— ä¸‹è¼‰å¤±æ•— ({idx+1}/{len(img_urls)})")
        
        time.sleep(0.5)  # é¿å…è«‹æ±‚å¤ªå¿«
    
    print(f"[åœ–ç‰‡] æˆåŠŸä¸‹è¼‰ {len(images_base64)}/{len(img_urls)} å¼µåœ–ç‰‡")
    
    # å»ºç«‹å•†å“è³‡æ–™
    shopify_product = {
        'product': {
            'title': translated['title'],
            'body_html': translated['description'],
            'vendor': 'The maple mania æ¥“ç³–ç”·å­©',
            'product_type': 'ã‚¯ãƒƒã‚­ãƒ¼ãƒ»æ´‹è“å­',
            'status': 'active',
            'published': True,
            'variants': [{
                'sku': product['sku'],
                'price': f"{selling_price:.2f}",
                'weight': product.get('weight', 0),
                'weight_unit': 'kg',
                'inventory_management': None,  # ä¸è¿½è¹¤åº«å­˜
                'inventory_policy': 'continue',  # å…è¨±è¶…è³£
                'requires_shipping': True
            }],
            'images': images_base64,
            'tags': 'The maple mania, æ¥“ç³–ç”·å­©, ãƒ¡ãƒ¼ãƒ—ãƒ«ãƒãƒ‹ã‚¢, æ—¥æœ¬, æ±äº¬, ä¼´æ‰‹ç¦®, æ±äº¬åœŸç”£, æ—¥æœ¬ä»£è³¼, æ¥“ç³–é¤…ä¹¾',
            'metafields_global_title_tag': translated['page_title'],
            'metafields_global_description_tag': translated['meta_description'],
            'metafields': [
                {
                    'namespace': 'custom',
                    'key': 'link',
                    'value': product['url'],
                    'type': 'url'
                }
            ]
        }
    }
    
    # ç™¼é€è«‹æ±‚
    response = requests.post(
        shopify_api_url('products.json'),
        headers=get_shopify_headers(),
        json=shopify_product
    )
    
    print(f"[DEBUG] Shopify å›æ‡‰: {response.status_code}")
    
    if response.status_code == 201:
        created_product = response.json()['product']
        product_id = created_product['id']
        variant_id = created_product['variants'][0]['id']
        
        # æª¢æŸ¥å¯¦éš›å»ºç«‹äº†å¹¾å¼µåœ–ç‰‡
        created_images = created_product.get('images', [])
        print(f"[DEBUG] å•†å“å»ºç«‹æˆåŠŸ: ID={product_id}")
        print(f"[DEBUG] Shopify å¯¦éš›å»ºç«‹åœ–ç‰‡: {len(created_images)}/{len(images_base64)} å¼µ")
        
        # æ›´æ–° variant çš„ cost (æˆæœ¬åƒ¹)
        update_cost_response = requests.put(
            shopify_api_url(f'variants/{variant_id}.json'),
            headers=get_shopify_headers(),
            json={
                'variant': {
                    'id': variant_id,
                    'cost': f"{cost:.2f}"
                }
            }
        )
        print(f"[DEBUG] æ›´æ–° Cost å›æ‡‰: {update_cost_response.status_code}")
        
        # åŠ å…¥ Collection
        if collection_id:
            add_product_to_collection(product_id, collection_id)
        
        # ç™¼å¸ƒåˆ°æ‰€æœ‰æ¸ é“
        publish_to_all_channels(product_id)
        
        return {'success': True, 'product': created_product, 'translated': translated, 'selling_price': selling_price, 'cost': cost}
    else:
        print(f"[ERROR] Shopify éŒ¯èª¤: {response.text}")
        return {'success': False, 'error': response.text}


# ========== Flask è·¯ç”± ==========

def run_scrape():
    """åŸ·è¡Œçˆ¬å–æµç¨‹"""
    global scrape_status
    
    try:
        scrape_status = {
            "running": True,
            "progress": 0,
            "total": 0,
            "current_product": "",
            "products": [],
            "errors": [],
            "uploaded": 0,
            "skipped": 0,
            "skipped_low_price": 0,
            "skipped_points": 0,
            "skipped_exists": 0,
            "filtered_by_price": 0,
            "deleted": 0
        }
        
        # 1. å–å¾—æˆ–å»ºç«‹ Collection
        scrape_status['current_product'] = "æ­£åœ¨è¨­å®š Collection..."
        collection_id = get_or_create_collection("The maple mania æ¥“ç³–ç”·å­©")
        print(f"[INFO] Collection ID: {collection_id}")
        
        # 2. å–å¾— Collection å…§çš„å•†å“ï¼ˆåªæª¢æŸ¥é€™å€‹ Collectionï¼‰
        scrape_status['current_product'] = "æ­£åœ¨å–å¾— Collection å…§å•†å“..."
        collection_products_map = get_collection_products_map(collection_id)
        existing_skus = set(collection_products_map.keys())
        print(f"[INFO] Collection å…§æœ‰ {len(existing_skus)} å€‹å•†å“")
        
        # 3. çˆ¬å–å•†å“åˆ—è¡¨
        scrape_status['current_product'] = "æ­£åœ¨çˆ¬å–å•†å“åˆ—è¡¨..."
        product_list = scrape_product_list()
        scrape_status['total'] = len(product_list)
        print(f"[INFO] æ‰¾åˆ° {len(product_list)} å€‹å•†å“")
        
        # å–å¾—å®˜ç¶²æ‰€æœ‰ SKU
        website_skus = set(item['sku'] for item in product_list)
        print(f"[INFO] å®˜ç¶² SKU åˆ—è¡¨: {len(website_skus)} å€‹")
        
        # 4. é€ä¸€è™•ç†å•†å“
        for idx, item in enumerate(product_list):
            scrape_status['progress'] = idx + 1
            scrape_status['current_product'] = f"è™•ç†ä¸­: {item['sku']}"
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if item['sku'] in existing_skus:
                print(f"[è·³é] å·²å­˜åœ¨: {item['sku']}")
                scrape_status['skipped_exists'] += 1
                scrape_status['skipped'] += 1
                continue
            
            # çˆ¬å–è©³ç´°è³‡è¨Š
            print(f"[çˆ¬å–] ({idx+1}/{len(product_list)}) {item['url']}")
            product = scrape_product_detail(item['url'])
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé»æ•¸å•†å“
            if product.get('is_points'):
                print(f"[è·³é] é»æ•¸å•†å“: {product.get('sku', item['sku'])}")
                scrape_status['skipped_points'] += 1
                scrape_status['skipped'] += 1
                continue
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºã€ŒãŠæ€¥ãä¾¿ã€å•†å“
            if 'ãŠæ€¥ãä¾¿' in product.get('title', ''):
                print(f"[è·³é] ãŠæ€¥ãä¾¿å•†å“: {product.get('title', item['sku'])}")
                scrape_status['skipped'] += 1
                continue
            
            # æª¢æŸ¥åƒ¹æ ¼é–€æª»
            if product.get('price', 0) < MIN_PRICE:
                print(f"[è·³é] åƒ¹æ ¼éä½: {product.get('title', item['sku'])} (Â¥{product.get('price', 0)})")
                scrape_status['skipped_low_price'] += 1
                scrape_status['filtered_by_price'] += 1
                scrape_status['skipped'] += 1
                continue
            
            # æª¢æŸ¥å¿…è¦è³‡è¨Š
            if not product.get('title') or not product.get('price'):
                print(f"[è·³é] è³‡è¨Šä¸å®Œæ•´: {item['sku']}")
                scrape_status['errors'].append({
                    'sku': item['sku'],
                    'error': 'è³‡è¨Šä¸å®Œæ•´'
                })
                continue
            
            # ä¸Šå‚³åˆ° Shopify
            result = upload_to_shopify(product, collection_id)
            
            if result['success']:
                translated_title = result.get('translated', {}).get('title', product['title'])
                print(f"[æˆåŠŸ] {translated_title}")
                scrape_status['uploaded'] += 1
                scrape_status['products'].append({
                    'sku': product['sku'],
                    'title': translated_title,
                    'original_title': product['title'],
                    'price': product['price'],
                    'selling_price': result.get('selling_price', 0),
                    'weight': product['weight'],
                    'status': 'success'
                })
            else:
                print(f"[å¤±æ•—] {product['title']}: {result['error']}")
                scrape_status['errors'].append({
                    'sku': product['sku'],
                    'title': product['title'],
                    'error': result['error']
                })
                scrape_status['products'].append({
                    'sku': product['sku'],
                    'title': product['title'],
                    'status': 'failed',
                    'error': result['error']
                })
            
            time.sleep(1)
        
        # 5. è¨­ç‚ºè‰ç¨¿ï¼šCollection å…§ä½†å®˜ç¶²å·²ä¸‹æ¶çš„å•†å“
        scrape_status['current_product'] = "æ­£åœ¨æª¢æŸ¥å·²ä¸‹æ¶å•†å“..."
        skus_to_draft = existing_skus - website_skus
        
        if skus_to_draft:
            print(f"[INFO] ç™¼ç¾ {len(skus_to_draft)} å€‹å•†å“éœ€è¦è¨­ç‚ºè‰ç¨¿")
            for sku in skus_to_draft:
                scrape_status['current_product'] = f"è¨­ç‚ºè‰ç¨¿: {sku}"
                product_id = collection_products_map.get(sku)
                if product_id and set_product_to_draft(product_id):
                    scrape_status['deleted'] += 1
                time.sleep(0.5)
        else:
            print(f"[INFO] æ²’æœ‰éœ€è¦è¨­ç‚ºè‰ç¨¿çš„å•†å“")
        
        scrape_status['current_product'] = "å®Œæˆï¼"
        
    except Exception as e:
        print(f"[ERROR] çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False


@app.route('/api/test-shopify')
def test_shopify():
    """æ¸¬è©¦ Shopify é€£ç·š"""
    if not load_shopify_token():
        return jsonify({'success': False, 'error': 'æœªè¨­å®šç’°å¢ƒè®Šæ•¸'})
    
    response = requests.get(
        shopify_api_url('shop.json'),
        headers=get_shopify_headers()
    )
    
    if response.status_code == 200:
        return jsonify({'success': True, 'shop': response.json()['shop']})
    else:
        return jsonify({'success': False, 'error': response.text}), 400


@app.route('/api/test-scrape')
def test_scrape():
    """æ¸¬è©¦çˆ¬å–å–®ä¸€å•†å“"""
    test_url = "https://sucreyshopping.jp/shop/g/gtmm01107/"
    product = scrape_product_detail(test_url)
    
    if product.get('price') and product.get('weight'):
        product['selling_price'] = calculate_selling_price(product['price'], product['weight'])
    
    return jsonify(product)


@app.route('/')
def index():
    """é¦–é """
    token_loaded = load_shopify_token()
    token_status = '<span style="color: green;">âœ“ å·²è¼‰å…¥</span>' if token_loaded else '<span style="color: red;">âœ— æœªè¨­å®š</span>'
    
    return f'''<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Maple Mania æ¥“ç³–ç”·å­© çˆ¬èŸ²å·¥å…·</title>
    <style>
        * {{ box-sizing: border-box; }}
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5; }}
        h1 {{ color: #333; border-bottom: 2px solid #8B4513; padding-bottom: 10px; }}
        .card {{ background: white; border-radius: 8px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .btn {{ background: #8B4513; color: white; border: none; padding: 12px 24px; border-radius: 5px; cursor: pointer; font-size: 16px; margin-right: 10px; }}
        .btn:hover {{ background: #6B3510; }}
        .btn:disabled {{ background: #ccc; cursor: not-allowed; }}
        .btn-secondary {{ background: #3498db; }}
        .progress-bar {{ width: 100%; height: 20px; background: #eee; border-radius: 10px; overflow: hidden; margin: 10px 0; }}
        .progress-fill {{ height: 100%; background: linear-gradient(90deg, #8B4513, #D2691E); transition: width 0.3s; }}
        .status {{ padding: 10px; background: #f8f9fa; border-radius: 5px; margin-top: 10px; }}
        .log {{ max-height: 300px; overflow-y: auto; font-family: monospace; font-size: 13px; background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 5px; }}
        .stats {{ display: flex; gap: 15px; margin-top: 15px; flex-wrap: wrap; }}
        .stat {{ flex: 1; min-width: 100px; text-align: center; padding: 15px; background: #f8f9fa; border-radius: 5px; }}
        .stat-number {{ font-size: 24px; font-weight: bold; color: #8B4513; }}
        .stat-label {{ font-size: 12px; color: #666; margin-top: 5px; }}
    </style>
</head>
<body>
    <h1>ğŸ The Maple Mania æ¥“ç³–ç”·å­© çˆ¬èŸ²å·¥å…·</h1>
    
    <div class="card">
        <h3>Shopify é€£ç·šç‹€æ…‹</h3>
        <p>Token: {token_status}</p>
        <button class="btn btn-secondary" onclick="testShopify()">æ¸¬è©¦é€£ç·š</button>
    </div>
    
    <div class="card">
        <h3>é–‹å§‹çˆ¬å–</h3>
        <p>çˆ¬å– sucreyshopping.jp The Maple Mania å•†å“ä¸¦ä¸Šæ¶åˆ° Shopify</p>
        <p style="color: #666; font-size: 14px;">â€» æˆæœ¬åƒ¹ä½æ–¼ Â¥1000ã€é»æ•¸å•†å“ã€ãŠæ€¥ãä¾¿å•†å“å°‡è‡ªå‹•è·³é</p>
        <button class="btn" id="startBtn" onclick="startScrape()">ğŸš€ é–‹å§‹çˆ¬å–</button>
        
        <div id="progressSection" style="display: none;">
            <div class="progress-bar">
                <div class="progress-fill" id="progressFill" style="width: 0%"></div>
            </div>
            <div class="status" id="statusText">æº–å‚™ä¸­...</div>
            
            <div class="stats">
                <div class="stat">
                    <div class="stat-number" id="uploadedCount">0</div>
                    <div class="stat-label">å·²ä¸Šæ¶</div>
                </div>
                <div class="stat">
                    <div class="stat-number" id="skippedCount">0</div>
                    <div class="stat-label">å·²è·³é</div>
                </div>
                <div class="stat">
                    <div class="stat-number" id="filteredCount">0</div>
                    <div class="stat-label">åƒ¹æ ¼éæ¿¾</div>
                </div>
                <div class="stat">
                    <div class="stat-number" id="deletedCount" style="color: #e67e22;">0</div>
                    <div class="stat-label">è¨­ç‚ºè‰ç¨¿</div>
                </div>
                <div class="stat">
                    <div class="stat-number" id="errorCount">0</div>
                    <div class="stat-label">éŒ¯èª¤</div>
                </div>
            </div>
        </div>
    </div>
    
    <div class="card">
        <h3>åŸ·è¡Œæ—¥èªŒ</h3>
        <div class="log" id="logArea">ç­‰å¾…é–‹å§‹...</div>
    </div>

    <script>
        let pollInterval = null;
        function log(msg, type = '') {{
            const logArea = document.getElementById('logArea');
            const time = new Date().toLocaleTimeString();
            const color = type === 'success' ? '#4ec9b0' : type === 'error' ? '#f14c4c' : '#d4d4d4';
            logArea.innerHTML += '<div style="color:' + color + '">[' + time + '] ' + msg + '</div>';
            logArea.scrollTop = logArea.scrollHeight;
        }}
        function clearLog() {{ document.getElementById('logArea').innerHTML = ''; }}
        async function testShopify() {{
            log('æ¸¬è©¦ Shopify é€£ç·š...');
            try {{
                const res = await fetch('/api/test-shopify');
                const data = await res.json();
                if (data.success) log('âœ“ é€£ç·šæˆåŠŸï¼', 'success');
                else log('âœ— é€£ç·šå¤±æ•—: ' + data.error, 'error');
            }} catch (e) {{ log('âœ— è«‹æ±‚å¤±æ•—: ' + e.message, 'error'); }}
        }}
        async function startScrape() {{
            clearLog(); log('é–‹å§‹çˆ¬å–æµç¨‹...');
            document.getElementById('startBtn').disabled = true;
            document.getElementById('progressSection').style.display = 'block';
            try {{
                const res = await fetch('/api/start-scrape', {{ method: 'POST' }});
                const data = await res.json();
                if (!data.success) {{ log('âœ— ' + data.error, 'error'); document.getElementById('startBtn').disabled = false; return; }}
                log('âœ“ çˆ¬å–ä»»å‹™å·²å•Ÿå‹•', 'success');
                pollInterval = setInterval(pollStatus, 1000);
            }} catch (e) {{ log('âœ— ' + e.message, 'error'); document.getElementById('startBtn').disabled = false; }}
        }}
        async function pollStatus() {{
            try {{
                const res = await fetch('/api/status');
                const data = await res.json();
                const percent = data.total > 0 ? (data.progress / data.total * 100) : 0;
                document.getElementById('progressFill').style.width = percent + '%';
                document.getElementById('statusText').textContent = data.current_product + ' (' + data.progress + '/' + data.total + ')';
                document.getElementById('uploadedCount').textContent = data.uploaded;
                document.getElementById('skippedCount').textContent = data.skipped;
                document.getElementById('filteredCount').textContent = data.filtered_by_price || data.skipped_low_price || 0;
                document.getElementById('deletedCount').textContent = data.deleted || 0;
                document.getElementById('errorCount').textContent = data.errors.length;
                if (!data.running && data.progress > 0) {{
                    clearInterval(pollInterval);
                    document.getElementById('startBtn').disabled = false;
                    log('========== çˆ¬å–å®Œæˆ ==========', 'success');
                }}
            }} catch (e) {{ console.error(e); }}
        }}
    </script>
</body>
</html>'''


@app.route('/api/status')
def get_status():
    """å–å¾—çˆ¬å–ç‹€æ…‹"""
    return jsonify(scrape_status)


@app.route('/api/start-scrape', methods=['POST'])
def start_scrape():
    """é–‹å§‹çˆ¬å–"""
    global scrape_status
    
    if scrape_status['running']:
        return jsonify({'success': False, 'error': 'çˆ¬å–æ­£åœ¨é€²è¡Œä¸­'})
    
    if not load_shopify_token():
        return jsonify({'success': False, 'error': 'æœªè¨­å®šç’°å¢ƒè®Šæ•¸'})
    
    thread = threading.Thread(target=run_scrape)
    thread.start()
    
    return jsonify({'success': True, 'message': 'é–‹å§‹çˆ¬å–'})


if __name__ == '__main__':
    print("=" * 50)
    print("The Maple Mania æ¥“ç³–ç”·å­© çˆ¬èŸ²å·¥å…·")
    print("=" * 50)
    
    port = int(os.environ.get('PORT', 8080))
    print(f"é–‹å•Ÿç€è¦½å™¨è¨ªå•: http://localhost:{port}")
    print("=" * 50)
    
    app.run(host='0.0.0.0', port=port, debug=False)
